{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a90933d3",
   "metadata": {},
   "source": [
    "# co-teaching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0d80cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8e2084d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FashionMNIST_0.3 dataset shapes:\n",
      "X_train: (18000, 784)\n",
      "y_train: (18000,)\n",
      "X_test: (3000, 784)\n",
      "y_test: (3000,)\n",
      "\n",
      "FashionMNIST_0.6 dataset shapes:\n",
      "X_train: (18000, 784)\n",
      "y_train: (18000,)\n",
      "X_test: (3000, 784)\n",
      "y_test: (3000,)\n",
      "\n",
      "CIFAR dataset shapes:\n",
      "X_train: (15000, 32, 32, 3)\n",
      "y_train: (15000,)\n",
      "X_test: (3000, 32, 32, 3)\n",
      "y_test: (3000,)\n"
     ]
    }
   ],
   "source": [
    "def load_datasets(base_path='C:\\\\Users\\\\natha\\\\Uni-Work (Github)\\\\Uni-Work\\\\2025 S2\\\\COMP5328\\\\Assignment 2\\\\datasets\\\\'):\n",
    "    \"\"\"\n",
    "    Load multiple datasets from .npz files\n",
    "    \n",
    "    Args:\n",
    "        base_path (str): Base directory path where datasets are stored\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing all loaded datasets\n",
    "    \"\"\"\n",
    "    datasets = {}\n",
    "    \n",
    "    # Define dataset configurations\n",
    "    dataset_configs = {\n",
    "        'FashionMNIST_0.3': 'FashionMNIST0.3.npz',\n",
    "        'FashionMNIST_0.6': 'FashionMNIST0.6.npz',\n",
    "        'CIFAR': 'CIFAR.npz'\n",
    "    }\n",
    "    \n",
    "    for dataset_name, filename in dataset_configs.items():\n",
    "        try:\n",
    "            # Load dataset\n",
    "            data = np.load(base_path + filename)\n",
    "            \n",
    "            # Extract data\n",
    "            datasets[dataset_name] = {\n",
    "                'X_train': data['Xtr'],\n",
    "                'y_train': data['Str'],\n",
    "                'X_test': data['Xts'],\n",
    "                'y_test': data['Yts']\n",
    "            }\n",
    "            \n",
    "            # Print shapes\n",
    "            print(f\"\\n{dataset_name} dataset shapes:\")\n",
    "            print(f\"X_train: {datasets[dataset_name]['X_train'].shape}\")\n",
    "            print(f\"y_train: {datasets[dataset_name]['y_train'].shape}\")\n",
    "            print(f\"X_test: {datasets[dataset_name]['X_test'].shape}\")\n",
    "            print(f\"y_test: {datasets[dataset_name]['y_test'].shape}\")\n",
    "            \n",
    "            # Close the dataset\n",
    "            data.close()\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: {filename} not found at {base_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {dataset_name}: {e}\")\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "# Load all datasets\n",
    "all_datasets = load_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "008ba013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(file, all_datasets):\n",
    "    \"\"\"\n",
    "    Preprocess FashionMNIST or CIFAR dataset\n",
    "    Returns: Xtr_t, ytr_t, Xts_t, yts_t, mean, std, is_cifar\n",
    "    \"\"\"\n",
    "    Xtr = all_datasets[file]['X_train']\n",
    "    ytr = all_datasets[file]['y_train']\n",
    "    Xts = all_datasets[file]['X_test']\n",
    "    yts = all_datasets[file]['y_test']\n",
    "    \n",
    "    is_cifar = not file.startswith('FashionMNIST')\n",
    "    \n",
    "    if file.startswith('FashionMNIST'):\n",
    "        # FashionMNIST: reshape to [N, 28, 28] and normalize\n",
    "        Xtr = Xtr.reshape(-1, 28, 28).astype(np.float32) / 255.0\n",
    "        Xts = Xts.reshape(-1, 28, 28).astype(np.float32) / 255.0\n",
    "        \n",
    "        # Compute stats\n",
    "        mean = Xtr.mean()\n",
    "        std = Xtr.std() + 1e-6\n",
    "        \n",
    "        # Convert to tensors with channel dimension [N, 1, 28, 28]\n",
    "        Xtr_t = torch.from_numpy(Xtr[:, None, :, :])\n",
    "        Xts_t = torch.from_numpy(Xts[:, None, :, :])\n",
    "        \n",
    "    else:  # CIFAR\n",
    "        # CIFAR: already [N, 32, 32, 3], normalize\n",
    "        Xtr = Xtr.astype(np.float32) / 255.0\n",
    "        Xts = Xts.astype(np.float32) / 255.0\n",
    "        \n",
    "        # Compute per-channel stats\n",
    "        mean = Xtr.mean(axis=(0, 1, 2))  # [3,]\n",
    "        std = Xtr.std(axis=(0, 1, 2)) + 1e-6  # [3,]\n",
    "        \n",
    "        # Convert to tensors: [N, H, W, C] -> [N, C, H, W]\n",
    "        Xtr_t = torch.from_numpy(Xtr).permute(0, 3, 1, 2)  # [N, 3, 32, 32]\n",
    "        Xts_t = torch.from_numpy(Xts).permute(0, 3, 1, 2)  # [N, 3, 32, 32]\n",
    "    \n",
    "    # Convert labels to tensors\n",
    "    ytr_t = torch.from_numpy(ytr.astype(np.int64))\n",
    "    yts_t = torch.from_numpy(yts.astype(np.int64))\n",
    "    \n",
    "    return Xtr_t, ytr_t, Xts_t, yts_t, mean, std, is_cifar\n",
    "\n",
    "def stratified_split_indices(y: torch.Tensor, train_frac=0.8, seed=42):\n",
    "    \"\"\"Stratified train/val split\"\"\"\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(seed)\n",
    "    y_np = y.cpu().numpy()\n",
    "    classes = np.unique(y_np)\n",
    "    train_idx, val_idx = [], []\n",
    "    for c in classes:\n",
    "        idx_c = np.flatnonzero(y_np == c)\n",
    "        idx_c = torch.as_tensor(idx_c)\n",
    "        # shuffle per class\n",
    "        perm = torch.randperm(idx_c.numel(), generator=g)\n",
    "        idx_c = idx_c[perm]\n",
    "        n_train_c = int(np.floor(train_frac * idx_c.numel()))\n",
    "        train_idx.append(idx_c[:n_train_c])\n",
    "        val_idx.append(idx_c[n_train_c:])\n",
    "    train_idx = torch.cat(train_idx).tolist()\n",
    "    val_idx = torch.cat(val_idx).tolist()\n",
    "    return train_idx, val_idx\n",
    "\n",
    "\n",
    "class ArrayDataset(Dataset):\n",
    "    def __init__(self, X, y, transform=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]\n",
    "        y = self.y[idx]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8df504cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: CIFAR\n",
      "Train shape: torch.Size([15000, 3, 32, 32]), Test shape: torch.Size([3000, 3, 32, 32])\n",
      "Is CIFAR: True\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 5. Complete Usage Example\n",
    "# ============================================================================\n",
    "\n",
    "# Configuration\n",
    "file = 'CIFAR'  # 'CIFAR' or 'FashionMNIST_0.3' or 'FashionMNIST_0.6'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Step 1: Preprocess dataset\n",
    "Xtr_t, ytr_t, Xts_t, yts_t, mean, std, is_cifar = preprocess_dataset(file, all_datasets)\n",
    "\n",
    "# Step 2: Create train_df with proper tensor data\n",
    "train_df = pd.DataFrame({\n",
    "    'X': [Xtr_t[i] for i in range(len(Xtr_t))],\n",
    "    'y': [ytr_t[i] for i in range(len(ytr_t))]\n",
    "})\n",
    "\n",
    "print(f\"Dataset: {file}\")\n",
    "print(f\"Train shape: {Xtr_t.shape}, Test shape: {Xts_t.shape}\")\n",
    "print(f\"Is CIFAR: {is_cifar}\")\n",
    "\n",
    "# Step 3: Stratified split\n",
    "train_idx, val_idx = stratified_split_indices(ytr_t, train_frac=0.8, seed=2025)\n",
    "\n",
    "# Step 4: Create datasets\n",
    "train_ds = ArrayDataset(Xtr_t[train_idx], ytr_t[train_idx])\n",
    "val_ds = ArrayDataset(Xtr_t[val_idx], ytr_t[val_idx])\n",
    "test_ds = ArrayDataset(Xts_t, yts_t)\n",
    "\n",
    "# Step 5: Create data loaders\n",
    "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=0, pin_memory=False)\n",
    "val_loader = DataLoader(val_ds, batch_size=256, shuffle=False, num_workers=0, pin_memory=False)\n",
    "test_loader = DataLoader(test_ds, batch_size=256, shuffle=False, num_workers=0, pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43a9eeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval function\n",
    "def eval_top1(model, loader, device):\n",
    "    model.eval()\n",
    "    total, correct = 0, 0\n",
    "    for x, y in loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        logits = model(x)\n",
    "        pred = logits.argmax(1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += y.size(0)\n",
    "    return 100.0 * correct / total\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.fc1 = nn.Linear(64*7*7, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))   # 28->14 for MNIST-like 28x28\n",
    "        x = self.pool(F.relu(self.conv2(x)))   # 14->7\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        logits = self.fc2(x)\n",
    "        return logits\n",
    "    \n",
    "class SimpleCNN_CIFAR(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        # Input: 3x32x32 (RGB)\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)   # -> 32x32x32\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)  # -> 64x32x32\n",
    "        self.pool = nn.MaxPool2d(2, 2)                # -> 64x16x16\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1) # -> 128x16x16\n",
    "        # After another pool: -> 128x8x8\n",
    "\n",
    "        self.fc1 = nn.Linear(128 * 8 * 8, 256)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # 32->16\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # 16->8\n",
    "        x = F.relu(self.conv3(x))             # Keep 8x8 spatial size\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        logits = self.fc2(x)\n",
    "        return logits\n",
    "\n",
    "def small_loss_indices(losses, keep_k):\n",
    "    # losses: [B], keep_k: int\n",
    "    _, idx = torch.topk(-losses, k=keep_k)  # smallest losses\n",
    "    return idx\n",
    "\n",
    "def get_keep_k(batch_size, epoch, max_epochs, noise_rate):\n",
    "    drop_rate = min(noise_rate * (epoch / max_epochs), noise_rate)\n",
    "    keep_frac = 1.0 - drop_rate\n",
    "    keep_k = max(1, int(keep_frac * batch_size))\n",
    "    return keep_k\n",
    "\n",
    "def train_epoch_coteaching(model_f, model_g, opt_f, opt_g, loader, epoch, max_epochs, noise_rate, device):\n",
    "    model_f.train(); model_g.train()\n",
    "    ce = nn.CrossEntropyLoss(reduction='none')\n",
    "    for x, y_noisy in loader:\n",
    "        x = x.to(device); y_noisy = y_noisy.to(device)\n",
    "        B = x.size(0)\n",
    "        keep_k = get_keep_k(B, epoch, max_epochs, noise_rate)\n",
    "\n",
    "        # Forward both models\n",
    "        logits_f = model_f(x)\n",
    "        logits_g = model_g(x)\n",
    "\n",
    "        # Per-sample losses\n",
    "        loss_f_i = ce(logits_f, y_noisy)     # [B]\n",
    "        loss_g_i = ce(logits_g, y_noisy)     # [B]\n",
    "\n",
    "        # Small-loss indices per model\n",
    "        idx_f = small_loss_indices(loss_f_i.detach(), keep_k)\n",
    "        idx_g = small_loss_indices(loss_g_i.detach(), keep_k)\n",
    "\n",
    "        # Each model updates on the peer's selected subset\n",
    "        loss_f_on_g = ce(logits_f[idx_g], y_noisy[idx_g]).mean()\n",
    "        loss_g_on_f = ce(logits_g[idx_f], y_noisy[idx_f]).mean()\n",
    "\n",
    "        opt_f.zero_grad(); loss_f_on_g.backward(); opt_f.step()\n",
    "        opt_g.zero_grad(); loss_g_on_f.backward(); opt_g.step()\n",
    "\n",
    "def run_coteaching(train_loader, num_classes=10, epochs=50, noise_rate=0.4, device='cuda'):\n",
    "    device = torch.device(device if torch.cuda.is_available() else 'cpu')\n",
    "    f = SimpleCNN(num_classes).to(device)\n",
    "    g = SimpleCNN(num_classes).to(device)\n",
    "    opt_f = torch.optim.SGD(f.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "    opt_g = torch.optim.SGD(g.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        train_epoch_coteaching(f, g, opt_f, opt_g, train_loader, ep, epochs, noise_rate, device)\n",
    "\n",
    "    return f, g\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b90ff0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "\n",
    "def run_coteaching_cv(dataset, n_splits=5, batch_size=128, num_classes=3,\n",
    "                      epochs=50, noise_rate=0.4, shuffle=True, seed=42, device='cuda',\n",
    "                      val_metric_fn=None, model_class=SimpleCNN):\n",
    "    \"\"\"\n",
    "    K-fold cross-validation for Co-teaching.\n",
    "    - dataset: a torch.utils.data.Dataset (map-style) returning (x, y_noisy)\n",
    "    - val_metric_fn: callable(model, val_loader, device) -> float; if None, returns None\n",
    "    Returns:\n",
    "      results: list of dicts per fold with keys {'fold', 'val_metric_f', 'val_metric_g', 'val_metric_ens'}\n",
    "      models: list of tuples (f, g) trained on each fold\n",
    "    \"\"\"\n",
    "    device = torch.device(device if torch.cuda.is_available() else 'cpu')\n",
    "    kf = KFold(n_splits=n_splits, shuffle=shuffle, random_state=seed)\n",
    "\n",
    "    # Create a proper dataset from the dataframe if needed\n",
    "    if hasattr(dataset, 'iloc'):  # It's a DataFrame\n",
    "        X_data = torch.stack([x for x in dataset['X']])\n",
    "        y_data = torch.stack([y for y in dataset['y']])\n",
    "        dataset = ArrayDataset(X_data, y_data)\n",
    "\n",
    "    results = []\n",
    "    models = []\n",
    "\n",
    "    indices = torch.arange(len(dataset))\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(indices)):\n",
    "        print(f\"\\nFold {fold + 1}/{n_splits}\")\n",
    "        \n",
    "        # DataLoaders for this fold\n",
    "        train_sampler = SubsetRandomSampler(indices[train_idx])\n",
    "        val_sampler   = SubsetRandomSampler(indices[val_idx])\n",
    "        train_loader  = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler, drop_last=True)\n",
    "        val_loader    = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)\n",
    "\n",
    "        # Fresh models per fold - use the correct SimpleCNN class\n",
    "        f = model_class(num_classes).to(device)\n",
    "        g = model_class(num_classes).to(device)\n",
    "        opt_f = torch.optim.SGD(f.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "        opt_g = torch.optim.SGD(g.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "        # Train\n",
    "        for ep in range(1, epochs+1):\n",
    "            train_epoch_coteaching(f, g, opt_f, opt_g, train_loader, ep, epochs, noise_rate, device)\n",
    "            if ep % max(1, epochs // 5) == 0:\n",
    "                print(f\"  Epoch {ep:02d} completed\")\n",
    "\n",
    "        # Optional validation on this fold\n",
    "        val_metric_f = eval_top1(f, val_loader, device) if val_metric_fn is not None else None\n",
    "        val_metric_g = eval_top1(g, val_loader, device) if val_metric_fn is not None else None\n",
    "\n",
    "        # Simple logit-averaged ensemble for validation\n",
    "        if val_metric_fn is not None:\n",
    "            class Ensemble(nn.Module):\n",
    "                def __init__(self, f, g):\n",
    "                    super().__init__(); self.f=f.eval(); self.g=g.eval()\n",
    "                def forward(self, x):\n",
    "                    return (self.f(x) + self.g(x)) / 2\n",
    "            ens = Ensemble(f, g).to(device)\n",
    "            val_metric_ens = eval_top1(ens, val_loader, device)\n",
    "        else:\n",
    "            val_metric_ens = None\n",
    "\n",
    "        print(f\"Fold {fold + 1} - Model F: {val_metric_f:.2f}%, Model G: {val_metric_g:.2f}%, Ensemble: {val_metric_ens:.2f}%\")\n",
    "\n",
    "        results.append({\n",
    "            'fold': fold,\n",
    "            'val_metric_f': val_metric_f,\n",
    "            'val_metric_g': val_metric_g,\n",
    "            'val_metric_ens': val_metric_ens\n",
    "        })\n",
    "        models.append((f, g))\n",
    "\n",
    "    # Print summary\n",
    "    if val_metric_fn is not None:\n",
    "        mean_f = np.mean([r['val_metric_f'] for r in results])\n",
    "        mean_g = np.mean([r['val_metric_g'] for r in results])\n",
    "        mean_ens = np.mean([r['val_metric_ens'] for r in results])\n",
    "        print(f\"\\nCross-validation summary:\")\n",
    "        print(f\"Model F: {mean_f:.2f}% ± {np.std([r['val_metric_f'] for r in results]):.2f}%\")\n",
    "        print(f\"Model G: {mean_g:.2f}% ± {np.std([r['val_metric_g'] for r in results]):.2f}%\")\n",
    "        print(f\"Ensemble: {mean_ens:.2f}% ± {np.std([r['val_metric_ens'] for r in results]):.2f}%\")\n",
    "\n",
    "    return results, models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "895d4ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1/5\n",
      "  Epoch 02 completed\n",
      "  Epoch 04 completed\n",
      "  Epoch 06 completed\n",
      "  Epoch 08 completed\n",
      "  Epoch 10 completed\n",
      "Fold 1 - Model F: 32.73%, Model G: 32.83%, Ensemble: 32.73%\n",
      "\n",
      "Fold 2/5\n",
      "  Epoch 02 completed\n",
      "  Epoch 04 completed\n",
      "  Epoch 06 completed\n",
      "  Epoch 08 completed\n",
      "  Epoch 10 completed\n",
      "Fold 2 - Model F: 32.77%, Model G: 32.77%, Ensemble: 32.77%\n",
      "\n",
      "Fold 3/5\n",
      "  Epoch 02 completed\n",
      "  Epoch 04 completed\n",
      "  Epoch 06 completed\n",
      "  Epoch 08 completed\n",
      "  Epoch 10 completed\n",
      "Fold 3 - Model F: 33.43%, Model G: 32.70%, Ensemble: 32.77%\n",
      "\n",
      "Fold 4/5\n",
      "  Epoch 02 completed\n",
      "  Epoch 04 completed\n",
      "  Epoch 06 completed\n",
      "  Epoch 08 completed\n",
      "  Epoch 10 completed\n",
      "Fold 4 - Model F: 34.90%, Model G: 36.07%, Ensemble: 34.83%\n",
      "\n",
      "Fold 5/5\n",
      "  Epoch 02 completed\n",
      "  Epoch 04 completed\n",
      "  Epoch 06 completed\n",
      "  Epoch 08 completed\n",
      "  Epoch 10 completed\n",
      "Fold 5 - Model F: 34.57%, Model G: 34.33%, Ensemble: 34.37%\n",
      "\n",
      "Cross-validation summary:\n",
      "Model F: 33.68% ± 0.90%\n",
      "Model G: 33.74% ± 1.31%\n",
      "Ensemble: 33.49% ± 0.92%\n"
     ]
    }
   ],
   "source": [
    "results, models = run_coteaching_cv(\n",
    "    dataset=train_df, n_splits=5, batch_size=128, num_classes=3,\n",
    "    epochs=10, noise_rate=0.3, shuffle=True, seed=42, device='cuda', val_metric_fn=True,\n",
    "    model_class=SimpleCNN_CIFAR if is_cifar else SimpleCNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db93dba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 - Model F: 33.33%, Model G: 40.67%, Ensemble: 33.53%\n",
      "Fold 2 - Model F: 33.33%, Model G: 33.33%, Ensemble: 33.33%\n",
      "Fold 3 - Model F: 33.30%, Model G: 33.33%, Ensemble: 32.70%\n",
      "Fold 4 - Model F: 37.23%, Model G: 51.63%, Ensemble: 38.53%\n",
      "Fold 5 - Model F: 36.37%, Model G: 35.57%, Ensemble: 35.63%\n",
      "\n",
      "Test Set Results (across 5 folds):\n",
      "Model F: 34.71% ± 1.73%\n",
      "Model G: 38.91% ± 6.90%\n",
      "Ensemble: 34.75% ± 2.13%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate all CV models on test set\n",
    "test_accuracies_f = []\n",
    "test_accuracies_g = []\n",
    "test_accuracies_ensemble = []\n",
    "\n",
    "for fold_idx, (model_f, model_g) in enumerate(models):\n",
    "    # Evaluate individual models\n",
    "    test_f = eval_top1(model_f, test_loader, device)\n",
    "    test_g = eval_top1(model_g, test_loader, device)\n",
    "    \n",
    "    # Create ensemble for this fold\n",
    "    class Ensemble(nn.Module):\n",
    "        def __init__(self, model_f, model_g):\n",
    "            super().__init__()\n",
    "            self.f = model_f.eval()\n",
    "            self.g = model_g.eval()\n",
    "        \n",
    "        def forward(self, x):\n",
    "            return (self.f(x) + self.g(x)) / 2\n",
    "    \n",
    "    ensemble_fold = Ensemble(model_f, model_g).to(device)\n",
    "    test_ensemble = eval_top1(ensemble_fold, test_loader, device)\n",
    "    \n",
    "    test_accuracies_f.append(test_f)\n",
    "    test_accuracies_g.append(test_g)\n",
    "    test_accuracies_ensemble.append(test_ensemble)\n",
    "    \n",
    "    print(f\"Fold {fold_idx + 1} - Model F: {test_f:.2f}%, Model G: {test_g:.2f}%, Ensemble: {test_ensemble:.2f}%\")\n",
    "\n",
    "# Calculate statistics\n",
    "mean_test_f = np.mean(test_accuracies_f)\n",
    "std_test_f = np.std(test_accuracies_f)\n",
    "mean_test_g = np.mean(test_accuracies_g)\n",
    "std_test_g = np.std(test_accuracies_g)\n",
    "mean_test_ensemble = np.mean(test_accuracies_ensemble)\n",
    "std_test_ensemble = np.std(test_accuracies_ensemble)\n",
    "\n",
    "print(f\"\\nTest Set Results (across {len(models)} folds):\")\n",
    "print(f\"Model F: {mean_test_f:.2f}% ± {std_test_f:.2f}%\")\n",
    "print(f\"Model G: {mean_test_g:.2f}% ± {std_test_g:.2f}%\")\n",
    "print(f\"Ensemble: {mean_test_ensemble:.2f}% ± {std_test_ensemble:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
